#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass paper
\begin_preamble
\usepackage{tikz}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\usepackage[hscale=0.7,vscale=0.8]{geometry}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language french
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language french
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Néguantropie, opacité et explicabilité des réseaux neuronaux artificiels
 profonds
\end_layout

\begin_layout Author
Johan Mathe johmathe@baylabs.io 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/sf_symphony_kubrick.jpg
	display false
	width 100col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
SF Symphony et 2001
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Je voudrais commencer par remercier Bernard Stiegler qui m'a demandé d'interveni
r pendant ce séminaire cet été apres une discussion sur la disruption au
 sein de l'entreprenariat dans la région de San Francisco.
 En tant qu'in génieur, j'ai vu ce projet comme un exercice de création
 d'un objet temporel.
 Je suis allé voir il y a plus d'un mois maintenant 2001 a space odyssey
 de Kubrick joué par l'orchestre symphonique de San Francisco.
 Ce film a toujours eu une place particuliere dans ma vie, je l'ai vu pour
 la premiere fois a l'age de 10 ans, puis tous les 5 ans jusqu'a aujourd'hui.
 C'etait donc mon cinquieme visionnage.
 Le fait d'avoir la musique jouée par un orchestre symphonique a mis en
 exergue la place centrale de strauss dans cette oeuvre de Kubrick, qui
 elle meme fait le pont vers Ainsi Parlait Zarathustra.
 J'ai donc décidé de travailler sous la contrainte pour cette présentation
 et d'utiliser une trame narrative qui va faire un paralele entre certains
 de mes travaux de recherche et l'analyse de 2001.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/2001_speculate.jpg
	display false
	width 100col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Citation de Kubrick
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Le premier volet du film montre un homme primitif confronté a la découverte
 de la premiere exosommatisation devant un monolithe noir.
 Kubrick fait alors une des ellipses narratives les plus longues de l'histoire.
 On arrive a ce qu'il considerera comme les dernieres exosommatisation:
 la conquete de l'espace et la naissance de l'intelligence artificielle.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/ape.png
	display false
	width 100col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Ape
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/vaisseau.png
	display false
	width 100col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Vaisseau
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Baylabs
\end_layout

\begin_layout Standard
Baylabs, Inc est une startup qui a pour mission d'améliorer la qualité,
 valeur et l'acces a l'imagerie médicale.
 Le premier produit sur lequel l'équipe travaille est un systeme d'analyse
 cardiaque basé sur l'utilisation de l'ultrason et de l'echocardiographie.
 Nous travaillons a l'interesection de deux technologies : la miniaturisation
 des techniques d'acquisition de données de scans ultrasons ainsi que le
 l'utilisation des réseaux neuronaux profonds.
 Nous travaillons actuellement en partenariat avec cinq universités et centres
 hospitaliers américains : Stanford University, Northwestern University
 a Chicago, Duke University sur la cote est ainsi que Mineapolis Heart Institute.
 Un de nos premiers prototypes permet de faire du diagnostic des signes
 a vant coureur de la fievre rhumatismale.
 La fièvre rhumatismale est une complication des infections de l’enfance
 et de l’adolescence, qui survient à la suite des angines dues au streptocoque
 hémolytique, et qui n’ont pas été soignées par des antibiotiques.
 Cette pathologie est assez facile a traiter avec des antibiotiques, mais
 difficile a diagnostiquer sans avoir de vision interne du coeur.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/baylabs_software.jpg
	display false
	width 100col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
John utilisant le prototype EchoMD
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Réseaux neuronaux artificiels profonds
\end_layout

\begin_layout Standard
Je vais maintenant introduire les réseaux de neurones artificiels profonds.
 Cette introduction est largement inspirée d'une parution en anglais 
\begin_inset CommandInset citation
LatexCommand cite
key "Goodfellow-et-al-2016-Book"

\end_inset

.
 Le but d'un réseau neuronal est d'approximer une fonction 
\begin_inset Formula $f^{*}$
\end_inset

.
 Un bon exemple une fonction de classification d'image 
\begin_inset Formula $y=f^{*}(x)$
\end_inset

 qui associe une classe de donnée a une catégorie y.
 Le réseau neuronal définit une application 
\begin_inset Formula $\boldsymbol{y}=f(\boldsymbol{x};\boldsymbol{W})$
\end_inset

 On ne mentionnera pas ici des réseau neuronaux récurrents.
 Ces modeles sont la base d'énormément d'applications.
 La définition de ces modeles est extremement symple.
 En effet il s'agit généralement de la composition de fonctions non-linéaires
 de la forme suivante:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(\boldsymbol{x})=f^{(n)}(\ldots f^{(i)}(\ldots f^{(1)}(\boldsymbol{x})))
\]

\end_inset


\end_layout

\begin_layout Standard
Généralement, les fonctions f ont la forme suivante, avec 
\begin_inset Formula $\sigma(x)$
\end_inset

 une fonction non linéaire comme par exemple l'unité linéaire rectifié (voir
 figure ):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f^{(i)}(\boldsymbol{x})=\sigma(\boldsymbol{W}_{i}^{T}\boldsymbol{x}+\boldsymbol{b_{i}})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Ces structures chainées peuvent etre vues comme un systeme multi couches.
 En effet la fonction 
\begin_inset Formula $f^{(1)}$
\end_inset

 représentera la premiere couche du réseau, jusqu'a 
\begin_inset Formula $f^{(n)}$
\end_inset

la derniere couche 
\begin_inset Formula $n$
\end_inset

 Le nombre de couche est appelé la 
\series bold
profondeur
\series default
 du réseau.
 Un modele graphique representant un reseau a deux couches:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
def
\backslash
layersep{2.5cm}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=
\backslash
layersep]  
\end_layout

\begin_layout Plain Layout


\backslash
tikzstyle{every pin edge}=[<-,shorten <=1pt]
\end_layout

\begin_layout Plain Layout


\backslash
tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
\end_layout

\begin_layout Plain Layout


\backslash
tikzstyle{input neuron}=[neuron, fill=green!50]    
\end_layout

\begin_layout Plain Layout


\backslash
tikzstyle{output neuron}=[neuron, fill=red!50]    
\end_layout

\begin_layout Plain Layout


\backslash
tikzstyle{hidden neuron}=[neuron, fill=blue!50]    
\end_layout

\begin_layout Plain Layout


\backslash
tikzstyle{annot} = [text width=4em, text centered]
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
foreach 
\backslash
name / 
\backslash
y in {1,...,4}    
\end_layout

\begin_layout Plain Layout

 
\backslash
node[input neuron, pin=left:Entrée 
\backslash
#
\backslash
y] (I-
\backslash
name) at (0,-
\backslash
y) {};
\end_layout

\begin_layout Plain Layout

  
\backslash
foreach 
\backslash
name / 
\backslash
y in {1,...,5}        
\end_layout

\begin_layout Plain Layout

   
\backslash
path[yshift=0.5cm]            
\end_layout

\begin_layout Plain Layout

    node[hidden neuron] (H1-
\backslash
name) at (
\backslash
layersep,-
\backslash
y cm) {};
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\backslash
foreach 
\backslash
name / 
\backslash
y in {1,...,5}        
\end_layout

\begin_layout Plain Layout

   
\backslash
path[yshift=0.5cm]            
\end_layout

\begin_layout Plain Layout

    node[hidden neuron] (H2-
\backslash
name) at (5cm,-
\backslash
y cm) {};
\end_layout

\begin_layout Plain Layout

 
\end_layout

\begin_layout Plain Layout

  
\backslash
foreach 
\backslash
source in {1,...,4}         
\end_layout

\begin_layout Plain Layout

   
\backslash
foreach 
\backslash
dest in {1,...,5}  
\end_layout

\begin_layout Plain Layout

    
\backslash
path (I-
\backslash
source) edge (H1-
\backslash
dest);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

 
\backslash
node[output neuron,pin={[pin edge={->}]right:Sortie}, right of=H2-3] (O)
 {};
\end_layout

\begin_layout Plain Layout

   
\backslash
foreach 
\backslash
source in {1,...,5}         
\end_layout

\begin_layout Plain Layout

    
\backslash
foreach 
\backslash
dest in {1,...,5}  
\end_layout

\begin_layout Plain Layout

     
\backslash
path (H1-
\backslash
source) edge (H2-
\backslash
dest);
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\backslash
foreach 
\backslash
source in {1,...,5}      
\end_layout

\begin_layout Plain Layout

   
\backslash
path (H2-
\backslash
source) edge (O);
\end_layout

\begin_layout Plain Layout

 
\backslash
node[annot,above of=H2-1, node distance=1cm] (hl) {Couche cachée~2 $f^{(2)}$};
   
\end_layout

\begin_layout Plain Layout

 
\backslash
node[annot,above of=H1-1, node distance=1cm] (hl) {Couche cachée~1 $f^{(1)}$};
   
\end_layout

\begin_layout Plain Layout

 
\backslash
node[annot,left of=hl] {Couche d'entrée};  
\end_layout

\begin_layout Plain Layout

 
\backslash
node[annot,right of=H2-1] {Couche de sortie}; 
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/non_linear.pdf
	display false
	width 45col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:non-linear"

\end_inset

Une non-linéarité de type relu
\begin_inset Formula $\sigma(x)=\max(0,x)$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
L'apprentissage
\begin_inset CommandInset label
LatexCommand label
name "subsec:L'apprentissage"

\end_inset


\end_layout

\begin_layout Standard
L'apprentissage est un processus souvent itératif qui permet d'estimer une
 fonction de type réseau neuronal grace a une grande quantité de données
 d'entrée.
 Dans notre cas de classification d'images pathologiques ou non, nous aurons
 un ensemble de tuples 
\begin_inset Formula $(X_{i},y_{i})$
\end_inset

 qui correspondront tout simplement aux images et a leurs labels, c'est
 a dire s'il s'agit d'un cas pathologique ou non.
\end_layout

\begin_layout Standard
D'un point de vue mathématique on peut voir le probleme de l'apprentissage
 comme un probleme d'optimisation.
 Il s'agit de minimiser la somme des erreur entre les prédictions de notre
 réseau de neurones et les données labellisées par des experts (dans notre
 cas les experts du monde médical).
 L'outil mathématique utilisé dans ce cas est une loss function TODO():
 traduire loss.
 Cette fonction a pour but d'augmenter quand l'erreur de prediction augmente,
 et de diminuer le cas contraire.
 
\end_layout

\begin_layout Standard
Un exemple classique d'une loss TODO translate est la distance euclidienne,
 aussi appelée loss euclidienne.
 Elle représente la distance euclidienne qui sépare 2 points dans un espace
 donné.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
l(\hat{y},y)=\Vert\hat{y}-y\Vert_{2}=\sqrt{\frac{1}{n}\sum_{i=1}^{N}(\hat{y_{i}}-y_{i})^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
Dans notre cas cet espace est l'espace des parametres et nous retrouvons
 donc avec la forme suivante:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mbox{\mbox{minimiser}} & J(\boldsymbol{\theta,y,X})=\sum_{i=1}^{N}\lVert y_{i}-f(X_{i};\boldsymbol{\theta}))\rVert_{2}^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
La minimisation de cette valeur se fait par l'algorithme du gradient (aussi
 appelé gradient descent, steepest descent).
 L'idée relativement simple.
 On commence par choisir un point aléatoire 
\begin_inset Formula $\boldsymbol{\theta}_{0}$
\end_inset

, puis on évalue la valeur du gradient 
\begin_inset Formula $\nabla_{\theta}J(\boldsymbol{\theta})$
\end_inset

 en ce point.
 Ce gradient représente un hyperplan dans l'espace a 
\begin_inset Formula $m$
\end_inset

 dimensions des parametres de notre fonction 
\begin_inset Formula $f(x;\boldsymbol{\theta})$
\end_inset

.
 On fait ensuite une mise a jour de notre parametre qui devient 
\begin_inset Formula $\boldsymbol{\theta}_{1}$
\end_inset

 en lui ajoutant une partie de ce gradient, pondéré par un taux d'apprentissage
 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{\theta}[k+1]=\boldsymbol{\theta}[k]+\alpha\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta,y,X})
\]

\end_inset


\end_layout

\begin_layout Standard
La figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "algo_illustr"

\end_inset

 illustre l'algorithme du gradient pour une fonction convexe quadratique.
 Dans ce cas la le gradient devient simplement la dérivée de la fonction
 en un point.
 Les droites rouges illustrent les tangentes de la droite en chaque point
 et montrent comment l'algorithme évolue.
 Apres quelques itérations on considere un critere d'arret comme par exemple
 le fait que la valeur du gradient soit proche d'une valeur arbitrairement
 faible 
\begin_inset Formula $\epsilon$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/gradient_descent.pdf
	display false
	width 50col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "algo_illustr"

\end_inset

Illustration de la méthode du gradient
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Convexité
\end_layout

\begin_layout Standard
Une des differences notables entre les algorithmes de type réseaux de neurones
 profonds et les méthodes plus anciennes d'apprentissage supervisé comme
 par exemple une SVM TODO citation SVM est la non-convexité du probleme.
 Nous allons revenir quelques instants sur la définition de la convexité,
 car celle ci est capitale pour la suite de cet exposé.
 L'ensemble convexe est défini de sorte a ce que si l'on choisit deux points
 
\begin_inset Formula $x,y$
\end_inset

 dans cet ensemble, tous les points formés par l'ensemble des barycentres
 entre ces deux points appartiendront aussi a l'ensemble (voir equation
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:convex_set"

\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\forall x,y\in C\quad\forall t\in[0,1]\qquad tx+(1-t)y\in C\label{eq:convex_set}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
La définition d'une fonction convexe est assez proche de celle de l'ensemble
 convexe (voir équation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:convex_fn"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f\left(tx+(1-t)y\right)\leq t\,f(x)+(1-t)\,f(y)\label{eq:convex_fn}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Une caracteristique principale des fonctions convexes est qu'elle ne possede
 qu'un seul maximum, celui ci étant donc de fait un maximum global.
 Ceci implique donc des garanties importantes quant a la convergence de
 notre fonction d'objectif.
 Nous avons la garantie d'obtenir la solution au probleme d'optimisation.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "48text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/convex.pdf
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Une fonction convexe de 
\begin_inset Formula $\mathbb{R}^{2}\mapsto\mathbb{R}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "48text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/non_convex.pdf
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Une fonction non convexe de 
\begin_inset Formula $\mathbb{R}^{2}\mapsto\mathbb{R}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Les réseaux neuronaux profonds etant composés d'une succession d'opérations
 linéaires (convexes) et non-linéaires (non convexes), la composition de
 fonctions non convexes est généralement une fonction non-convexe.
 Ceci implique que dans le cas général, les solutions obtenues par des algorithm
es de la famille des algorithmes du gradient ne nous donnent pas une garantie
 sur l'optimalité et la globalité du résultat trouvé.
\end_layout

\begin_layout Subsection
Une nouvelle fonction d'erreur : minimisation de l'entropie croisée
\end_layout

\begin_layout Standard
On veut quantifier l'information d'une maniere qui formalise une forme d'intuiti
on.
 
\end_layout

\begin_layout Itemize
Les evenements qui sont fortement probable devraient avoir un contenu en
 information faible, et les evements qui sont garantis devraient avoir un
 contenu informationel proche de zéro.
 Un exemple typique est que le soleil se leverea demain.
 Cette phrase a un contenu informationel faible si l'on connait l'histoire
 des levers et couchers de soleils depuis le début de l'histoire de l'humanité.
\end_layout

\begin_layout Itemize
Les evenements peu probables devraient avoir un contenu informationel élevé.
 Les évemenents independants devraient avoir une information additive.
 Par exemple, se rendre compte que lors d'un lancer de dés est tombé sur
 pile deux fois
\end_layout

\begin_layout Standard
Shannon a théorisé dans son papier 
\series bold
A Mathematical Theory of Communication 
\series default
l'entropie comme suit: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
I(x)=-\log P(x)
\]

\end_inset


\end_layout

\begin_layout Standard
Différents types d'entropie:
\end_layout

\begin_layout Itemize
Entropy de gibbs
\end_layout

\begin_layout Itemize
Entropy de von neumann
\end_layout

\begin_layout Standard
Self-information deals only with a single outcome.
 We can quantify the amountof uncertainty in an entire probability distribution
 using the Shannon entropy:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(x)\mathbb{=E}_{x\sim P}[I(x)]=-\mathbb{E}_{X\sim P}[\log(x)]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C=-\frac{1}{n}\sum_{x}[y\ln a+(1-y)\ln(1-a)]
\]

\end_inset


\end_layout

\begin_layout Subsection
Entropie Croisée
\end_layout

\begin_layout Standard
Shannon a théorisé l'entropie d'un point de vue informationel dans sa publicatio
n de 1948, 
\series bold
A Mathematical Theory of Communication 
\begin_inset CommandInset citation
LatexCommand cite
key "Shannon:1963:MTC:1102016"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H_{y'}(y)=\sum_{i}y_{i}'\log(y_{i})
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Opacité, Explicabilité
\end_layout

\begin_layout Standard
Dans 2001 l'odyssée de l'espace, le kubrick met en scene un monolithe a
 trois reprises, dans des moments clés de l'intrigue:
\end_layout

\begin_layout Itemize
Au debut juste avant que le post neanderthal mette en place sa premiere
 exosomatisation
\end_layout

\begin_layout Itemize
Juste avant l'apparition de HAL9000, qui est finalement l'exosommatisaion
 ultime
\end_layout

\begin_layout Itemize
A la fin, pour la transition entre l'homme et l'ubermensch
\end_layout

\begin_layout Standard
Dans l'oeuvre originale de l'ecrivain Arthur C Clarke de qui le film est
 inspiré, le monolithe a une forme de pyramide.
 Kubrick décide de remplacer cette forme pyramidale en forme parallépipedique,
 qui ressemble finalement a une boite noire.
 Quand Kubrick dans les années 60 a fait ses recherches sur les balbutiements
 de l'intelligence artificielle, il a passé du temps avec Minsky au MIT,
 pour essayer de comprendre les tenants et les aboutissants de l'IA.
 En théorie des systemes, une boite noire est une représentation d'un systeme
 pour lequel seulement ses entrées et ses sorties sont observables.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "48text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/monolithe1.png
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Premiere apparition du Monolithe
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "48text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/monolithe2.png
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Deuxieme apparition du Monolithe
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "48text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/monolithe3.png
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
troisieme aparition du Monolithe
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "48text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/monolithe-sun.png
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Monolithe
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Je vais maintenant décrire des travaux qui sont liés a des axes de recherches
 actifs et récents.
 En particulier les problématiques d'explicabilité et d'opacité.
 J'utilise ici la définition suivante de l'opacité d'un réseau de neurones
 artificiel:
\end_layout

\begin_layout Standard
Un réseau de neurone artificiel est dit opaque de par la nature non-convexe
 du probleme d'apprentissage, nous n'avons aucune visibilité sur les performance
s ni sur les criteres internes qui font que ce dernier perform (TODO) bien
 ou mal TODO rephrase.
\end_layout

\begin_layout Standard
L'explicabilité ici est définie comme une production d'information qui va
 déterminer les criteres d'un signal d'entrée qui ont amené le réseau de
 neurones a faire un choix plutot qu'un autre.
\end_layout

\begin_layout Subsection
Opacité
\end_layout

\begin_layout Standard
Une des plus grandes problématiques posées aujourd'hui par les réseaux de
 neurones profonds est liée a ce qu'on peut définir comme l'opacité et le
 manque d'explicabilitié de ceux-cis.
\end_layout

\begin_layout Standard
Pour diminuer l'opacité de ce type d'algorithmes, nous nous penchons sur
 une technique relativement ancienne, mais qui a été re popularisée récemment
 grace aux travaux de Erhan et al 2009 TODO Citations.
 Il s'agit simplement de l'operation inverse de l'apprentissage.
 Nous avons vu que l'apprentissage consistait simplement en une optimisation
 d'une fonction d'objectif visant a minimiser une erreur de prédiction,
 ou encore une entropie croisée entre une distribution attendue et une distribut
ion produite par le réseau de neurones.
\end_layout

\begin_layout Standard
Ici nous allons maximiser une activation en considerant la fonction inverse
 de notre reseau de neurones:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h^{*}(\boldsymbol{X},\boldsymbol{\theta})=f^{*-1}(\boldsymbol{X},\boldsymbol{\theta})
\]

\end_inset


\end_layout

\begin_layout Standard
Nous solvons donc le probleme d'optimisation suivant:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{X^{*}}=\mbox{argmax}h(\boldsymbol{\theta},\boldsymbol{X})
\]

\end_inset


\end_layout

\begin_layout Standard
Nous pouvons réutiliser la méthode de l'algorithme du gradient présentée
 lors de l'introduction de l'apprentissage (
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:L'apprentissage"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/rhd_positive.jpg
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Un cas pathologique de fievre rhumatique
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/opacite1.jpg
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Analyse de l'opacité, exemple 1
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/opacite2.jpg
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Analyse de l'opacité, exemple 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Explicabilité
\end_layout

\begin_layout Standard
Ici pour l'explicabilité, nous utilisons les approches de zieler et fergus
 TODO citation fergus 2014 qui consiste a observer la classification des
 resultats d'un réseau de neurone deja entrainé dans le cas ou une image
 aura une zone de taille 
\begin_inset Formula $(n,n)$
\end_inset

 pixels qui sera 
\begin_inset Quotes fld
\end_inset

effacée
\begin_inset Quotes frd
\end_inset

 TODO: image avec patch effacé.
 On déplace ensuite cette image d'un pixel de sorte a créer une heatmap
 (TODO: translate)
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/rhd_positive.jpg
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Un cas pathologique de fievre rhumatique
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/heatmap_alone.png
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Analyse de l'opacité, exemple 1
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/heatmap.png
	display false
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Analyse de l'opacité, exemple 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Faux positifs, qu'est ce qu'on extrait de ces images?
\end_layout

\begin_layout Section
Néguantropie, prolétarisation, circuits longs de la transindividuation
\end_layout

\begin_layout Standard
Une des motivations pour ce travail de recherche et développement autour
 de l'explicabilité et de l'opacité au sein de Bay Labs vient de plusieurs
 discussions avec les profesionnels du corps médical.
 Quand nous avons commencé a travailler avec les cardiologues sur ces questions
 d'apprentissage et de réseaux neuronaux, un de leur retours principaux
 était la question de la prolétarisation.
 En effet en tant que chercheurs, ingénieurs et entrepreneurs, nous fournissons
 un outil aux médecins en vue de produire un diagnostic.
 Il est critique que le médecin qui a la responsabilité finale du diagnostic
 comprenne pourquoi l'outil fournit tel ou tel diagnostique.
\end_layout

\begin_layout Standard
De plus, depuis le début de l'aventure, nous tentons de penser les circuits
 longs de la transindividuation entre le corps médical et notre équipe de
 recherche.
 C'est a mon sens l'utilisation continue de ces boucles de renforcement
 entre les médecins qui peuvent plus que jamais utiliser leur esprit critique.
 En effet si nous conservons la trace des évemenents bugs on peut faire
 bien.
 Aussi nous avons remarqué que dans certains cas eg afrique et biais
\end_layout

\begin_layout Standard
Finalement Kubrick positionne la boite noire - le monollithe - avant chaque
 transition.
 Ces boites noires agissent comme des marqueurs de temps.
\end_layout

\begin_layout Itemize
Dans le premier cas, nous avons du recul pour comprendre le fonctionnement
 d'un outil aussi primaire que le baton.
 Il nous montre notre passé.
 Il montre aussi que l'homme primitif ne comprennait pas vraiment la mecanique
 classique.
 Pour nous l'utilisation de cet outil est tout a fait transparente.
 Pour le singe c'est une boite noire.
\end_layout

\begin_layout Itemize
Dans le deuxieme cas, kubrick montre les dangers de designer une AI de facon
 boite noire.
 En effet la gestion des erreurs devient impossible (la crisis commence
 au moment ou les ingenieurs a bord se rendent compte que HAL9000 a commit
 une erreur.
 La confiance est perdue.)
\end_layout

\begin_layout Itemize
Dans le dernier cas, totalement oenirique, kubrick a mon sens théorise la
 singularité (voir figure ) en empruntant la thématique de l'ubermensch
 de Nietzche.
 Nietzche définit l'ubermescnh come l'home supérieur pouvant s'élever en
 dessus de la morale chrétienne et imposer ses propres valeurs.
 Il nous met face a un scénario qui dépassent notre entendement cartésien
 et joue sur nos peur primaires (vieillissement prématuré de l'acteur dans
 un environnement vicié) - avant d'avoir la renaissance finale.
 Avec le theme de strauss reprenant le relai.
 On se retrouve dans un cas typique Kubrick ne compose plus seulement avec
 la musique et l'image, mais avec le subconscient du récepteur.
 La renaissance de l'homme en 
\begin_inset Quotes fld
\end_inset

star child
\begin_inset Quotes frd
\end_inset

.
 La boite noire est la pour nous rapeller que les techniques sont souvent
 découvertes par hasard.
 L'homme singe n'a pas attendu la mécanique newtonienne pour se servir du
 baton - hors c'est celle ci qui va expliquer avec exactitude quelle energie
 il devra déployer pour l'utiliser précisément.
\end_layout

\begin_layout Standard
Ce que je propose ici, c'est que la néguanthropie passe avant tout par l'augment
ation de l'explicabilité de la techné.
 Cela passe par l'utilisation des circuits longs de la transindividuation
 inter-domaine.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/singularity.png
	display false
	width 100col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
proposition : Singularité selon Kubrick
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/ubermensch.png
	display false
	width 100col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
proposition : ubermesnch
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Une proposition pharmacologique
\end_layout

\begin_layout Standard
Je propose d'utiliser une approche pharmacologique lors de l'apprentissage.
 Rapellons nous 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mbox{\mbox{minimiser}} & J(\boldsymbol{\theta,y,X})=\sum_{i=1}^{N}\lVert y_{i}-f(X_{i};\boldsymbol{\theta}))\rVert_{2}^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Maintenant si nous definissons une fonction d'explicabilitié 
\begin_inset Formula $E(\theta)$
\end_inset

 et une fonction d'opacité 
\begin_inset Formula $O(\theta)$
\end_inset

, nous pouvons définir une fonction d'erreur pharmakologique 
\begin_inset Formula $J_{p}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mbox{\mbox{minimiser}} & J_{p}(\boldsymbol{\theta},\boldsymbol{y},\boldsymbol{X})=J(\boldsymbol{\theta},\boldsymbol{y},\boldsymbol{X})+\alpha O(\boldsymbol{\theta})-\beta E(\boldsymbol{\theta})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
J'aimerais que nous discutions ensemble d'un type d'algorithme que je définis
 comme socio inspiré.
 En effet il nous incompe de définir ces fonctions O et E en s'inspirant
 des sciences de l'éducation et de l'aprentissage et de la pédagogice
\end_layout

\begin_layout Standard
Science vs explicabilitié: 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "dl"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
